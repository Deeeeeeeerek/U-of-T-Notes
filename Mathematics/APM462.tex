\documentclass[12pt]{article}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathabx}
\usepackage{graphicx}
\usepackage{listings}
\newenvironment{sol}
{\renewcommand\qedsymbol{}\begin{proof}[Solution]}{\end{proof}}
\renewcommand {\thefigure} {\thesection{}.\arabic{figure}}
\theoremstyle{definition}
\newtheorem{eg}{Example}[section]
\theoremstyle{definition}
\newtheorem{thm}{Theorem}[section]
\theoremstyle{definition}
\newtheorem{lemma}{Lemma}
\theoremstyle{definition}
\newtheorem{prop}{Property}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\begin{document}

\title{Nonlinear Optimization}
\author{Derek Li}
\date{}
\maketitle

\tableofcontents
\newpage

\section{Review}
\subsection{One-Variable Calculus}
\begin{thm}[Mean Value Theorem]
	Let $g \in C^1$ on $\mathbb{R}.$ We have $$\frac{g(x+h)-g(x)}{h}=g'(x+\theta h),$$ for some $\theta\in(0, 1)$ and $\frac{g(x+h)-g(x)}{h}$ is the slope of secant line between $(x, g(x))$ and $(x+h, g(x+h)).$ Or we can write $g(x+h)=g(x)+hg'(x+\theta h).$
\end{thm}

\begin{thm}[First Order Taylor Approximation]
	Let $g \in C^1$ on $\mathbb{R}.$ We have $$g(x+h)=g(x)+hg'(x)+o(h),$$ where $o(h)$ is the error and we say a function $f(h)=o(h)$ to mean $$\lim_{h\to0}\frac{f(h)}{h}=0.$$
\end{thm}
\begin{proof}
	Want to show $g(x+h)-g(x)-hg'(x)=o(h).$
	\\\\ We have $$\begin{aligned}\lim_{h\to0}\frac{g(x+h)-g(x)-hg'(x)}{h}&=\lim_{h\to0}\frac{hg'(x+\theta h)-hg'(x)}{h}\\&=\lim_{h\to0}g'(x+\theta h)-g'(x)=0.\end{aligned}$$
\end{proof}

\begin{thm}[Second Order Mean Value Theorem]
	Let $g \in C^2$ on $\mathbb{R}.$ We have $$g(x+h)=g(x)+hg'(x)+\frac{h^2}{2}g''(x+\theta h),$$ for some $\theta\in(0, 1).$
\end{thm}

\begin{thm}[Second Order Taylor Approximation]
	Let $g \in C^2$ on $\mathbb{R}.$ We have $$g(x+h)=g(x)+hg'(x)+\frac{h^2}{2}g''(x)+o(h^2).$$
\end{thm}
\begin{proof}
	W.T.S. $g(x+h)-g(x)-hg'(x)-\frac{h^2}{2}g''(x)=o(h^2).$
	\\\\ We have $$\begin{aligned}\lim_{h\to0}\frac{g(x+h)-g(x)-hg'(x)-\frac{h^2}{2}g''(x)}{h^2}&=\lim_{h\to0}\frac{\frac{h^2}{2}g''(x+\theta h)-\frac{h^2}{2}g''(x)}{h^2}\\&=\lim_{h\to0}\frac{1}{2}[g''(x+\theta h)-g''(x)]=0.\end{aligned}$$
\end{proof}

\subsection{Multi-variable Calculus}
\begin{definition}[Gradient]
	Gradient of $f: \mathbb{R}^n \to \mathbb{R}$ at $\mathbf{x}\in\mathbb{R}^n, \nabla f(\mathbf{x}),$ if exists is a vector characterized by the property $$\lim_{\mathbf{v}\to\mathbf{0}}\frac{f(\mathbf{x}+\mathbf{v})-f(\mathbf{x})-\nabla f(\mathbf{x})\cdot\mathbf{v}}{\|\mathbf{v}\|}=0,$$ and $\nabla f(\mathbf{x})=\left(\frac{\partial f}{\partial x_1}(\mathbf{x}), \cdots, \frac{\partial f}{\partial x_n}(\mathbf{x})\right).$
\end{definition}

\noindent The instantaneous rate of change of $f$ at $\mathbf{x}$ in direction $\mathbf{v}$ (suppose w.l.o.g. $\|\mathbf{v}\|=1$) is $$\begin{aligned}\frac{\mathrm{d}}{\mathrm{d}t}\bigg|_{t=0}f(\mathbf{x}+t\mathbf{v})&=\nabla f(\mathbf{x}+t\mathbf{v})\cdot\mathbf{v}|_{t=0}\\&=\nabla f(\mathbf{x})\cdot\mathbf{v}\\&=|\nabla f(\mathbf{x})||\mathbf{v}|\cos\theta\\&=|\nabla f(\mathbf{x})|\cos\theta,\end{aligned}$$ where $\theta$ is the angle between $\nabla f(\mathbf{x})$ and $\mathbf{v}.$ Obviously, the instantaneous rate maximizes when $\theta=0.$ Therefore, when it is not equal to zero, $\nabla f(\mathbf{x})$ points in the direction of steepest ascent.

\begin{thm}[Mean Value Theorem in $\mathbb{R}^n$]
	Let $f \in C^1$ on $\mathbb{R}^n,$ then for any $\mathbf{x}, \mathbf{v} \in \mathbb{R}^n,$ we have $$f(\mathbf{x}+\mathbf{v})=f(\mathbf{x})+\nabla f(\mathbf{x}+\theta\mathbf{v})\cdot\mathbf{v},$$ for some $\theta\in(0, 1).$
\end{thm}
\begin{proof}
	Consider $g(t)=f(\mathbf{x}+t\mathbf{v}),$ where $t\in\mathbb{R}$ and $g \in C^1$ on $\mathbb{R}.$
	\\\\ By Mean Value Theorem in $\mathbb{R},$ we have $$\begin{aligned}g(0+1)&=g(0)+1 \cdot g'(0+\theta\cdot1)\\&=g(0)+g'(\theta)\\&=f(\mathbf{x})+\nabla f(\mathbf{x}+\theta\mathbf{v})\cdot\mathbf{v}\\&=g(1)=f(\mathbf{x}+\mathbf{v}),\end{aligned}$$ for some $\theta\in(0, 1).$
\end{proof}
\noindent \emph{Note}. $$g'(t)=\frac{\mathrm{d}}{\mathrm{d}t}f(\mathbf{x}+t\mathbf{v})=\nabla f(\mathbf{x}+t\mathbf{v})\cdot\mathbf{v}.$$

\begin{thm}[First Order Taylor Approximation in $\mathbb{R}^n$]
	Let $f \in C^1$ on $\mathbb{R}^n.$ We have $$f(\mathbf{x}+\mathbf{v})=f(\mathbf{x})+\nabla f(\mathbf{x})\cdot\mathbf{v}+o(\|\mathbf{v}\|).$$
\end{thm}
\begin{proof}
We have $$\begin{aligned}\lim_{\|\mathbf{v}\|\to0}\frac{f(\mathbf{x}+\mathbf{v})-f(\mathbf{x})-\nabla f(\mathbf{x})\cdot\mathbf{v}}{\|\mathbf{v}\|}&=\lim_{\|\mathbf{v}\|\to0}\frac{\nabla f(\mathbf{x}+\theta\mathbf{v})\cdot\mathbf{v}-\nabla f(\mathbf{x})\cdot\mathbf{v}}{\|\mathbf{v}\|}\\&=\lim_{\|\mathbf{v}\|\to0}[\nabla f(\mathbf{x}+\theta\mathbf{v})-\nabla f(\mathbf{v})]\cdot\frac{\mathbf{v}}{\|\mathbf{v}\|}=0.\end{aligned}$$
\end{proof}

\begin{thm}[Second Order Mean Value Theorem in $\mathbb{R}^n$]
	Let $f \in C^2$ on $\mathbb{R}^n.$ We have $$f(\mathbf{x}+\mathbf{v})=f(\mathbf{x})+\nabla f(\mathbf{x})\cdot\mathbf{v}+\frac{1}{2}\mathbf{v}^T\nabla^2f(\mathbf{x}+\theta\mathbf{v})\cdot\mathbf{v},$$ for some $\theta\in(0, 1).$
\end{thm}
\noindent \emph{Note 1}. Hessian matrix $$\nabla^2f(\mathbf{x})=\left(\frac{\partial^2f}{\partial x_i \partial x_j}(\mathbf{x})\right)_{1 \leq i, j \leq n}$$ is a symmetric matrix because of Clairaut's Theorem.
\\\\ \emph{Note 2}. $$\mathbf{v}^T\nabla^2f(\mathbf{x})\cdot\mathbf{v}=\sum_{1 \leq i, j \leq n}\frac{\partial^2f}{\partial x_i \partial x_j}(\mathbf{x})v_iv_j.$$

\begin{thm}[Second Order Taylor Approximation in $\mathbb{R}^n$]
	Let $f \in C^2$ on $\mathbb{R}^n.$ We have $$f(\mathbf{x}+\mathbf{v})=f(\mathbf{x})+\nabla f(\mathbf{x})\cdot\mathbf{v}+\frac{1}{2}\mathbf{v}^T\nabla^2f(\mathbf{x})\cdot\mathbf{v}+o(\|\mathbf{v}\|^2).$$
\end{thm}
\begin{proof}
We have $$\begin{aligned}&\ \ \ \ \lim_{\|\mathbf{v}\|\to0}\frac{f(\mathbf{x}+\mathbf{v})-f(\mathbf{x})-\nabla f(\mathbf{x})\cdot\mathbf{v}-\frac{1}{2}\mathbf{v}^T\nabla^2f(\mathbf{x})\cdot\mathbf{v}}{\|\mathbf{v}\|^2}\\&=\lim_{\|\mathbf{v}\|\to0}\frac{\frac{1}{2}\mathbf{v}^T\nabla^2f(\mathbf{x}+\theta\mathbf{v})\cdot\mathbf{v}-\frac{1}{2}\mathbf{v}^T\nabla^2f(\mathbf{x})\cdot\mathbf{v}}{\|\mathbf{v}\|^2}\\&=\lim_{\|\mathbf{v}\|\to0}\frac{1}{2}\left(\frac{\mathbf{v}}{\|\mathbf{v}\|}\right)^T\cdot[\nabla^2f(\mathbf{x}+\theta\mathbf{v})-\nabla^2f(\mathbf{x})]\cdot\frac{\mathbf{v}}{\|\mathbf{v}\|}\\&=0.\end{aligned}$$
\end{proof}

\begin{thm}[Implicit Function Theorem]
	Let $f: \mathbb{R}^{n+1}\to\mathbb{R}$ be a $C^1$ function. Fix $(\mathbf{a}, b)\in\mathbb{R}^n \times \mathbb{R}$ s.t. $f(\mathbf{a}, b)=0$ and $\nabla f(\mathbf{a}, b)\neq0.$ We have $\{(\mathbf{x}, y)\in\mathbb{R}^n\times\mathbb{R}|f(\mathbf{x}, y)=0\}$ is locally the graph of a function.
\end{thm}

\begin{definition}[Level Set]
	$\{\mathbf{x}\in\mathbb{R}^n|f(\mathbf{x})=c\}$ is called $c$-level set of $f.$
\end{definition}

\begin{thm}
	Gradient $\nabla f(\mathbf{x}_0) \perp$ level curve through $\mathbf{x}_0.$
\end{thm}

\begin{definition}[Convex Set]
	$\Omega \subseteq \mathbb{R}^n$ is a convex set if for all $\mathbf{x}_1, \mathbf{x}_2 \in \Omega,$ we have line segment between $\mathbf{x}_1, \mathbf{x}_2: s\mathbf{x}_1+(1-s)\mathbf{x}_2 \in \Omega, s \in [0, 1].$
\end{definition}

\begin{definition}[Convex Function]
	A function $f: \Omega \subseteq \mathbb{R}^n \to \mathbb{R}$ is convex if $$f(s\mathbf{x}_1+(1-s)\mathbf{x}_2) \leq sf(\mathbf{x}_1)+(1-s)f(\mathbf{x}_2),$$ for all $\mathbf{x}_1, \mathbf{x}_2 \in \Omega$ and all $s \in [0, 1],$ where $\Omega$ is a convex set.
\end{definition}

\begin{definition}[Concave Function]
	A function $f$ concave if $-f$ is convex.
\end{definition}

\noindent \emph{Note}. The linear function is both convex and concave.

\end{document}